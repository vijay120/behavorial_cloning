{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import cv2\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "\n",
    "mainDir = '/home/carnd/data/'\n",
    "secondaryDir = '/home/carnd/more_data_2/'\n",
    "\n",
    "df = pd.read_csv(mainDir + 'driving_log.csv', sep=',', header=0)\n",
    "\n",
    "## data I obtained by driving around the lap\n",
    "df2 = pd.read_csv(secondaryDir + 'driving_log.csv', sep=',', header=0)\n",
    "\n",
    "## combine the two datasets\n",
    "df = df.append(df2, ignore_index=True)\n",
    "\n",
    "inbet = df[(df[\"steering\"] < 0.05) & (df[\"steering\"] > -0.05)]\n",
    "out = df[(df[\"steering\"] > 0.05) | (df[\"steering\"] < -0.05)]\n",
    "\n",
    "## remove data with small steering angels\n",
    "df = pd.concat([out, inbet.sample(frac=0.5, replace=False)])\n",
    "\n",
    "train_samples, validation_samples = train_test_split(df, test_size=0.2)\n",
    "\n",
    "def generator(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            \n",
    "            X = batch_samples[\"center\"]\n",
    "            Left = batch_samples[\"left\"]\n",
    "            Right = batch_samples[\"right\"]\n",
    "            \n",
    "            X_Regular = X.map(lambda img : cv2.imread(mainDir + img.strip().replace(\"/Users/vijaytramakrishnan/more_data_2/\", \"\"), 1))\n",
    "            Y_Regular = batch_samples[\"steering\"]\n",
    "\n",
    "            X_Left = Left.map(lambda img : cv2.imread(mainDir + img.strip().replace(\"/Users/vijaytramakrishnan/more_data_2/\", \"\"), 1))\n",
    "            Y_Left = batch_samples[\"steering\"] + 0.2\n",
    "\n",
    "            X_Right = Right.map(lambda img : cv2.imread(mainDir + img.strip().replace(\"/Users/vijaytramakrishnan/more_data_2/\", \"\"), 1))\n",
    "            Y_Right = batch_samples[\"steering\"] - 0.2\n",
    "\n",
    "            X_Reg_flipped = X.map(lambda img : cv2.flip(cv2.imread(mainDir + img.strip().replace(\"/Users/vijaytramakrishnan/more_data_2/\", \"\"), 1), 1))\n",
    "            Y_Reg_flipped = Y_Regular.map(lambda measurement: -1.0 * measurement)\n",
    "\n",
    "            X_Left_flipped = Left.map(lambda img : cv2.flip(cv2.imread(mainDir + img.strip().replace(\"/Users/vijaytramakrishnan/more_data_2/\", \"\"), 1), 1))\n",
    "            Y_Left_flipped = Y_Left.map(lambda measurement: -1.0 * measurement)\n",
    "\n",
    "            X_Right_flipped = Right.map(lambda img : cv2.flip(cv2.imread(mainDir + img.strip().replace(\"/Users/vijaytramakrishnan/more_data_2/\", \"\"), 1), 1))\n",
    "            Y_Right_flipped = Y_Right.map(lambda measurement: -1.0 * measurement)\n",
    "\n",
    "            X_concat = pd.concat([X_Regular, X_Left, X_Right, X_Reg_flipped, X_Left_flipped, X_Right_flipped])\n",
    "            Y_concat = pd.concat([Y_Regular, Y_Left, Y_Right, Y_Reg_flipped, Y_Left_flipped, Y_Right_flipped])\n",
    "            X_concat = np.asarray(X_concat.tolist())\n",
    "            Y_concat = np.asarray(Y_concat.tolist())\n",
    "                        \n",
    "            yield shuffle(X_concat, Y_concat)    \n",
    "    \n",
    "train_generator = generator(train_samples, batch_size=128)\n",
    "validation_generator = generator(validation_samples, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8329\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(secondaryDir + 'driving_log.csv',newline='') as f:\n",
    "    r = csv.reader(f)\n",
    "    data = [line for line in r]\n",
    "    \n",
    "with open(secondaryDir + 'driving_log.csv','w',newline='') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"center\",\"left\",\"right\",\"steering\",\"throttle\",\"brake\",\"speed\"])\n",
    "    w.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "6144/6663 [==========================>...] - ETA: 1s - loss: 0.0587"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py:1569: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6912/6663 [===============================] - 22s - loss: 0.0576 - val_loss: 0.0468\n",
      "Epoch 2/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0461 - val_loss: 0.0413\n",
      "Epoch 3/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0337 - val_loss: 0.0317\n",
      "Epoch 4/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0289 - val_loss: 0.0256\n",
      "Epoch 5/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0292 - val_loss: 0.0268\n",
      "Epoch 6/30\n",
      "6954/6663 [===============================] - 17s - loss: 0.0273 - val_loss: 0.0204\n",
      "Epoch 7/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0259 - val_loss: 0.0259\n",
      "Epoch 8/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0247 - val_loss: 0.0207\n",
      "Epoch 9/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0249 - val_loss: 0.0243\n",
      "Epoch 10/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0262 - val_loss: 0.0256\n",
      "Epoch 11/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0232 - val_loss: 0.0200\n",
      "Epoch 12/30\n",
      "6954/6663 [===============================] - 16s - loss: 0.0244 - val_loss: 0.0237\n",
      "Epoch 13/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0247 - val_loss: 0.0250\n",
      "Epoch 14/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0229 - val_loss: 0.0196\n",
      "Epoch 15/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0238 - val_loss: 0.0256\n",
      "Epoch 16/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0238 - val_loss: 0.0196\n",
      "Epoch 17/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0227 - val_loss: 0.0196\n",
      "Epoch 18/30\n",
      "6954/6663 [===============================] - 16s - loss: 0.0251 - val_loss: 0.0197\n",
      "Epoch 19/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0235 - val_loss: 0.0168\n",
      "Epoch 20/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0225 - val_loss: 0.0236\n",
      "Epoch 21/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0228 - val_loss: 0.0190\n",
      "Epoch 22/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0236 - val_loss: 0.0245\n",
      "Epoch 23/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0220 - val_loss: 0.0247\n",
      "Epoch 24/30\n",
      "6954/6663 [===============================] - 16s - loss: 0.0230 - val_loss: 0.0183\n",
      "Epoch 25/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0219 - val_loss: 0.0218\n",
      "Epoch 26/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0221 - val_loss: 0.0238\n",
      "Epoch 27/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0237 - val_loss: 0.0182\n",
      "Epoch 28/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0219 - val_loss: 0.0248\n",
      "Epoch 29/30\n",
      "6954/6663 [===============================] - 16s - loss: 0.0222 - val_loss: 0.0198\n",
      "Epoch 30/30\n",
      "6912/6663 [===============================] - 16s - loss: 0.0220 - val_loss: 0.0189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7bb4f52080>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, Cropping2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "dropout = 0.2\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(160,320,3)))\n",
    "model.add(Cropping2D(cropping=((70,25), (0,0))))\n",
    "\n",
    "model.add(Convolution2D(24, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "model.add(Convolution2D(36, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "model.add(Convolution2D(48, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "model.add(Convolution2D(64, 3, 3, subsample=(1, 1), activation='relu'))\n",
    "model.add(Convolution2D(64, 3, 3, subsample=(1, 1), activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1164, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile('adam', 'mse')\n",
    "model.fit_generator(train_generator, \n",
    "                    samples_per_epoch=len(train_samples), \n",
    "                    validation_data=validation_generator,\n",
    "                    nb_val_samples=len(validation_samples), \n",
    "                    nb_epoch=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('model20.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:carnd-term1]",
   "language": "python",
   "name": "conda-env-carnd-term1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
